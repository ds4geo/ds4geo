{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Session 9.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ds4geo/ds4geo/blob/master/WS%202020%20Course%20Notes/Session%209.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0mAVyPptn_q0"
      },
      "source": [
        "# **Data Science for Geoscientists - Winter Semester 2020**\n",
        "# **Session 9 - Supervised Machine Learning - 2nd December 2020**\n",
        "\n",
        "This week we will use two supervised machine learning techniques to try and classify rock types based on their bulk chemistry. We will use the traditional random forest method, and a simple neural network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7OECHvUzOrW"
      },
      "source": [
        "# 9.1 Introduction to Supervised Machine Learning\n",
        "* Learning classification or regression from data with labels\n",
        "* Aim to learn general rules which can be applied on other data\n",
        "* Many algorithms are black boxes - the \"rules\" are difficult or impossible to know or understand\n",
        "\n",
        "* examples\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3In9XSx41fCa"
      },
      "source": [
        "# 9.2 Introduction to rock geochem database\n",
        "To try out some supervised ML, we use the rock geochemistry database published here:\n",
        "https://essd.copernicus.org/articles/11/1553/2019/essd-11-1553-2019.pdf\n",
        "\n",
        "We will try to use ML to predict the rock types based on the bulk chemistry.\n",
        "I've already prepared and cleaned a subset of the data containing about 300,000 rock samples with rock names (top 14 only) and major element data:\n",
        "\n",
        "https://github.com/ds4geo/ds4geo/blob/master/data/unordered/gwrgdb_maj_ele.csv?raw=true"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWDUJRNd5Mk6"
      },
      "source": [
        "## Inspecting the data\n",
        "Below we:\n",
        "* Load the data\n",
        "* Do some simple visualisations\n",
        "* Perform PCA analysis and standardisation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "biHqhLNy7kRH"
      },
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQYpCOKf7l3j"
      },
      "source": [
        "dat = pd.read_csv(\"https://github.com/ds4geo/ds4geo/blob/master/data/unordered/gwrgdb_maj_ele.csv?raw=true\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YeYymkaH9a3L"
      },
      "source": [
        "# dat.rock_name.value_counts()\n",
        "fig, ax = plt.subplots(figsize=(10,7))\n",
        "sns.countplot(data=dat, y=\"rock_name\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RXJLyvo99157"
      },
      "source": [
        "# Plot Al vs Si as scatter plot\n",
        "fig, ax = plt.subplots(figsize=(12,10))\n",
        "sns.scatterplot(data=dat, x=\"al2o3\", y=\"sio2\", hue=\"rock_name\", ax=ax, palette=\"tab10\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3QGhUDy7-tza"
      },
      "source": [
        "# We'll want to standardise the data\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUN4j_pm-_C8"
      },
      "source": [
        "# Standardize\n",
        "rdat = dat.iloc[:,1:]\n",
        "sdat = StandardScaler().fit_transform(X=rdat)\n",
        "sdat = pd.DataFrame(sdat, index=rdat.index, columns=rdat.columns)\n",
        "sdat"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "20Iwazm8-veq"
      },
      "source": [
        "# Lets do a PCA to see if there's clear structure\n",
        "from sklearn.decomposition import PCA"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0QCwv7T---Df"
      },
      "source": [
        "pca = PCA()\n",
        "pca.fit(sdat)\n",
        "\n",
        "pdat = pca.transform(sdat)\n",
        "pdat = pd.DataFrame(pdat, index=sdat.index)\n",
        "evr = pca.explained_variance_ratio_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hvXMViFmAdui"
      },
      "source": [
        "from sklearn import preprocessing\n",
        "# convert rock names to integer labels - necessary later for classification\n",
        "le = preprocessing.LabelEncoder()\n",
        "y = le.fit_transform(dat.rock_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2x5qeDY_8YN"
      },
      "source": [
        "fig, ax = plt.subplots(figsize=(10,10))\n",
        "pc2plot = (1,2)\n",
        "ax.scatter(pdat.loc[:, pc2plot[0]-1], pdat.loc[:, pc2plot[1]-1], alpha=0.2, c=[\"C{}\".format(j) for j in y])\n",
        "ax.axis('equal')\n",
        "ax.set_xlabel(\"PC{} - explained variance: {}%\".format(pc2plot[0], round(evr[pc2plot[0]-1]*100,1)))\n",
        "ax.set_ylabel(\"PC{} - explained variance: {}%\".format(pc2plot[1], round(evr[pc2plot[1]-1]*100,1)))\n",
        "\n",
        "for v, nm in zip(pca.components_[[np.array(pc2plot)-1],:].T, sdat.columns):\n",
        "  vec = v * 12.5\n",
        "  ax.plot([0,vec[0]], [0, vec[1]],\"r\")\n",
        "  ax.text(vec[0],vec[1], nm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DcYDfAQR1fdA"
      },
      "source": [
        "# 9.3 Machine Learning Algorithms: Random Forest\n",
        "Here we try out a common machine learning classification method: random forest, and explain shortly how it works."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UmGHa-63kPYf"
      },
      "source": [
        "## ML models in python\n",
        "At simplest, all these algorithms have the same usage style in python:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oRBl3IXhpyA8"
      },
      "source": [
        "# We'll discuss this step later\n",
        "from sklearn import model_selection\n",
        "xt, xv, yt, yv = model_selection.train_test_split(sdat,y, test_size=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1y2yuS8okNm-"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "# create the model - we'll discuss the parameters later\n",
        "clf = RandomForestClassifier(max_depth=15, random_state=0, verbose=1, n_jobs=5)\n",
        "\n",
        "# train the model on input data and corresponding labels\n",
        "# This step takes some minutes, so we'll let it run while we continue with the explanations\n",
        "clf.fit(xt, yt)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9FJUY33_p_0L"
      },
      "source": [
        "# Score the model to assess its accuracy\n",
        "clf.score(xt, yt)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tAIiDgn2qW9o"
      },
      "source": [
        "# Use the model for prediction\n",
        "clf.predict(xv)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgoWu1Hjqpye"
      },
      "source": [
        "## Theory: Decision Trees and Random Forest\n",
        "* Decision trees for classification\n",
        " * A hierarchical set of rules to classify from attributes/data\n",
        "\n",
        " * Rock classification example:\n",
        "![](https://www.vagabondgeology.com/uploads/3/4/1/2/3412046/2367852.jpg?895)\n",
        "\n",
        "  * Can be learnt from the data itself\n",
        "  * Start with the feature/attribute which best splits the data\n",
        "\n",
        "* Generalisation and the problem of overfitting\n",
        " * Aim is to learn the general patterns from the data\n",
        " * But ML models can end up simply memorising the training data, not learning the general patterns\n",
        " * This is overfitting\n",
        " * Decision trees are very prone to overfitting\n",
        "* The solution: Random Forest\n",
        " * An ensemble of a large number of deliberately imperfect decision trees.\n",
        " * Randomly pick features/attributes at each level of the tree\n",
        " * Average results of all the decision trees in the forest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BzeErOn4liCT"
      },
      "source": [
        "## Train, Validate, Test\n",
        "Because ML models should learn general patterns from the data, its important to see how they perform on data which they do not train on. Standard practice is to split the available data into 2-3 groups:\n",
        "1. Training data - data the model directly uses while learning.\n",
        "2. Validation data - data the model uses periodically to test its own performance, but which isn't seen while learning.\n",
        "3. Test data - the gold standard - data which the model has never even indirectly interacted with which provides a completely unbiased assessment of its performance.\n",
        "\n",
        "See above: train_test_split()\n",
        "\n",
        "Below we compare the performance of our model on the training and validation data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCKTcSgpofvD"
      },
      "source": [
        "clf.verbose = False # turn off progress messages"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pnTxTY5xoOOC"
      },
      "source": [
        "print(\"train score:\", round(clf.score(xt, yt)*100,1),\"%\")\n",
        "print(\"test score:\", round(clf.score(xv, yv)*100,1),\"%\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffEjUiEeolz-"
      },
      "source": [
        "We can see that the model performs much better on the training data than the test data, but that it performs well on both!\n",
        "\n",
        "This is an indication of slight overfitting. Dramatic overfitting can occur when train scores are very high (approaching 100%) but test scores approach 0%."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQSciKIjntOW"
      },
      "source": [
        "## Model hyperparameters\n",
        "ML models have parameters which define their properties which depend on the type of model.\n",
        "For random forest, two key parameters are:\n",
        "1. The number of trees in the forest (n_estimators)- how large the ensemble is\n",
        "2. The maximum tree depth (max_depth) - how many levels of decision can occur per tree\n",
        "\n",
        "The above example used 100 trees (the default) and a maximum tree depth of 15.\n",
        "Below we try some other combinations:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "th25XLRMoH_r"
      },
      "source": [
        "# 15 trees with max depth of 5\n",
        "clf_small = RandomForestClassifier(n_estimators=10, max_depth=5, random_state=0, verbose=1, n_jobs=5)\n",
        "clf_small.fit(xt, yt)\n",
        "print(\"train score:\", round(clf_small.score(xt, yt)*100,1),\"%\")\n",
        "print(\"test score:\", round(clf_small.score(xv, yv)*100,1),\"%\")\n",
        "# result very fast (only a few seconds), but much lower accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ouSESWj3qEqK"
      },
      "source": [
        "# 50 trees with max depth of 5\n",
        "clf_med1 = RandomForestClassifier(n_estimators=50, max_depth=5, random_state=0, verbose=1, n_jobs=5)\n",
        "clf_med1.fit(xt, yt)\n",
        "print(\"train score:\", round(clf_med1.score(xt, yt)*100,1),\"%\")\n",
        "print(\"test score:\", round(clf_med1.score(xv, yv)*100,1),\"%\")\n",
        "# More trees didn't help accuracies, but took much longer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k9HeQICmqRPI"
      },
      "source": [
        "# 10 trees with max depth of 15\n",
        "clf_med2 = RandomForestClassifier(n_estimators=10, max_depth=15, random_state=0, verbose=1, n_jobs=5)\n",
        "clf_med2.fit(xt, yt)\n",
        "print(\"train score:\", round(clf_med2.score(xt, yt)*100,1),\"%\")\n",
        "print(\"test score:\", round(clf_med2.score(xv, yv)*100,1),\"%\")\n",
        "# Maximum depth seems to help accuracies a lot more, even with fewer trees, making it faster than the original case!"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zUeVJVAfqfAV"
      },
      "source": [
        "# 10 trees with max depth of 25\n",
        "clf_med2 = RandomForestClassifier(n_estimators=10, max_depth=25, random_state=0, verbose=1, n_jobs=5)\n",
        "clf_med2.fit(xt, yt)\n",
        "print(\"train score:\", round(clf_med2.score(xt, yt)*100,1),\"%\")\n",
        "print(\"test score:\", round(clf_med2.score(xv, yv)*100,1),\"%\")\n",
        "# Increasing maximum depth even with few estimators increases the test score, but more (but not bad) overfitting!"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCAqGIccpMHp"
      },
      "source": [
        "## Assessing the Model Performance - Confusion Matrix\n",
        "Assessing ML model performance is itself a huge topic, but one useful approach is the confusion matrix. It compares all possible combinations of true categories and predicted categories. Correct classifications are on the diagonal. It is valuable for seeing which categories the model struggles to classify.\n",
        "\n",
        "Fortunately, sklearn provides a very easy all-in-one function to make confusion matrix plots.\n",
        "\n",
        "Given the list of rock types, which confusions do you expect?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PD8OSYjHrfah"
      },
      "source": [
        "print(le.classes_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Ji5s3MjrFhf"
      },
      "source": [
        "from sklearn.metrics import plot_confusion_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhp1QXOOpnMk"
      },
      "source": [
        "fig, ax = plt.subplots(figsize=(14,14))\n",
        "plot_confusion_matrix(clf, xv, yv, normalize=\"true\", display_labels=le.classes_, ax=ax, cmap=\"BuGn\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHLKW86grrSv"
      },
      "source": [
        "## Hyper parameter search\n",
        "It is common practice to automatically test different hyperparameter combinations to find a set which produce the most accurate model.\n",
        "We try a simple search here just on the max_depth parameter."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ydEySL38sHjv"
      },
      "source": [
        "# import helpful progress bar library\n",
        "from tqdm.notebook import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9Zrv4jyr49r"
      },
      "source": [
        "train_score = []\n",
        "val_score = []\n",
        "depth = []\n",
        "for j in tqdm(range(5,30)):\n",
        "    depth.append(j)\n",
        "    \n",
        "    clf = RandomForestClassifier(n_estimators=5, max_depth=j, random_state=0, verbose=0, n_jobs=8)\n",
        "    clf.fit(xt, yt)\n",
        "    \n",
        "    train_score.append(clf.score(xt, yt))\n",
        "    val_score.append(clf.score(xv, yv))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qo8FVn6Mr8ZM"
      },
      "source": [
        "# Compare the scores vs tree depths\n",
        "fig, ax = plt.subplots(figsize=(10,7))\n",
        "ax.plot(depth,train_score, label=\"training score\")\n",
        "ax.plot(depth,val_score, label=\"validation score\")\n",
        "ax.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WAP2ILKcpn0U"
      },
      "source": [
        "# 9.4 Neural Networks and Deep Learning\n",
        "\n",
        "![](https://imgs.xkcd.com/comics/machine_learning.png)\n",
        "\n",
        "Neural networks are ML models designed to work analogously to neurons in the brain. Each neuron is connected to every neuron in the layer before and the layer after and each connection has a weighting.\n",
        "\n",
        "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/9/99/Neural_network_example.svg/1200px-Neural_network_example.svg.png\"  height=\"600\" />\n",
        "\n",
        "\n",
        "In the simplest possible terms, a neural network learns by randomly changing all of the weights, seeing if the output is closer or further away from the true data (i.e. the labels), and iteratively refining these weights until the learning doesn't improve further.\n",
        "\n",
        "We will discuss in class more next week, but please watch the following videos to understand the general concepts:\n",
        "A really light introduction (note, the \"infinite classrom\" learning approach here isn't a good analogy for the actual way neural networks usually learn, but it is a helpful concept to understand anyway):\n",
        "https://www.youtube.com/watch?v=R9OHn5ZF4Uo\n",
        "\n",
        "A footnote to the above video which very lightly introduces the real way neural networks learn:\n",
        "https://www.youtube.com/watch?v=wvWpdrfoEv0\n",
        "\n",
        "Taking a serious but excellent step into what is really going on:\n",
        "https://www.youtube.com/watch?v=aircAruvnKk\n",
        "See also the follow-up videos in that series.\n",
        "\n",
        "\n",
        "sklearn makes it very easy to switch different ML models, so we can try using a deep neural network with very few changes, and leave greater understanding of what is really going on for next week.\n",
        "\n",
        "First we replicate the original Random Forest example as closely as possible.\n",
        "\n",
        "Note, during training, if we set verbose=1, we see the \"loss\" of the model for each iteration of training. The loss is a measure of model performance like accuracy, but where lower is better. Many loss functions are available depending on the data and problem."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u0ksk8vSvFL9"
      },
      "source": [
        "from sklearn.neural_network import MLPClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p51WmPJevIkm"
      },
      "source": [
        "mlp = MLPClassifier(hidden_layer_sizes=(32), verbose=1, max_iter=50)\n",
        "mlp.fit(xt, yt)\n",
        "\n",
        "print(\"train score:\", mlp.score(xt, yt))\n",
        "print(\"test score:\", mlp.score(xv, yv))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-g2DFs2vfi5"
      },
      "source": [
        "## Deep learning\n",
        "Deep learning is poorly defined, but roughly, it refers to neural networks with at least 2 hidden layers (i.e. excluding the input and output layers).\n",
        "\n",
        "Lets try a very simple deep neural network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EibbKgxtvv0P"
      },
      "source": [
        "mlp = MLPClassifier(hidden_layer_sizes=(32, 32), verbose=1, max_iter=50)\n",
        "mlp.fit(xt, yt)\n",
        "\n",
        "print(\"train score:\", mlp.score(xt, yt))\n",
        "print(\"test score:\", mlp.score(xv, yv))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PY9b8ATq16Dx"
      },
      "source": [
        "Next steps decided by class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5rZP6nI2AqW"
      },
      "source": [
        "# 9.5 Main Project\n",
        "To be discussed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VxByBSBs1PYD"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}