{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DS4GEO_L5.ipynb",
      "provenance": [],
      "private_outputs": true,
      "toc_visible": true,
      "authorship_tag": "ABX9TyNDI3wfKp0E0JnFuLS45UOS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ds4geo/ds4geo/blob/master/WS%202020%20Course%20Notes/Session%205.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcdW54CLeSyH"
      },
      "source": [
        "# **Data Science for Geoscientists - Winter Semester 2020**\n",
        "# **Session 5 - Time Series - 4th November 2020**\n",
        "In the previous sessions and assignments, we've covered basic data handling, maniuplation and visualisation. In this session we will go deeper into working with time series, especially two frequently encountered topics: interpolation and filtering. We will use a monitoring dataset from Spanagel cave."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uE7qVuiWebgJ"
      },
      "source": [
        "# Part 5.1 - Walkthrough of Session 4 LA-ICPMS excercise - *Walkthrough/Discussion*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVLAWrSYebwR"
      },
      "source": [
        "# Part 5.2 - Cave monitoring data excercise part 1 - *Workshop*\n",
        "We will work with cave monitoring data from Spanagel cave collected by Paul TÃ¶chterle and the UIBK Quaternary Research Group.\n",
        "\n",
        "The data was collected to understand the cave circulation which is important for interpreting speleothem based palaeoclimate records from the cave. Three types of data were collected: 1. outside air temperature, 2. cave temperature, and 3. cave CO2 concentration, oxygen and carbon isotope ratios.\n",
        "\n",
        "The following poster gives an explanation of the project and the data: https://github.com/ds4geo/ds4geo/blob/master/data/timeseries/Spanagel_Poster.pdf\n",
        "\n",
        "We will load the data and perform some important processing steps to enable further comparative analysis of the three datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G28TpnTbyIUf"
      },
      "source": [
        "## Part 5.2.1 - Load the cave monitoring data\n",
        "Three datasets are available: Cave temperature, Outside air temperature, and Cave CO2 measurements (concentration, d13C, d18O).\n",
        "\n",
        "The data sets are located as follows:\n",
        "* Cave air temperature:\n",
        " * \"https://github.com/ds4geo/ds4geo/raw/master/data/timeseries/Au%C3%9Fenluft%2BEingangslabyrinth.xlsx\"\n",
        " * Sheet: \"Daten3\"\n",
        " * Time column: A, data column: G\n",
        "* Outside air temperature:\n",
        " * \"https://github.com/ds4geo/ds4geo/raw/master/data/timeseries/Au%C3%9Fenluft%2BEingangslabyrinth.xlsx\"\n",
        " * Sheet: \"Daten3\"\n",
        " * Time column: I, data column: J\n",
        "* Cave CO2 measurements:\n",
        " * https://github.com/ds4geo/ds4geo/raw/master/data/timeseries/CO2%20_compiled.xlsx\"\n",
        " * Sheet: \"Data Stream (2)\n",
        " * Time column: A, d13C column: C, d18O column: D, ppm CO2 column: E\n",
        "\n",
        "**Task**\n",
        "Load all three datasets using Pandas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kY-jL4T7yN_q"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMedLSvQSvhm"
      },
      "source": [
        "##Part 5.2.2 - Plot the data to get an overview\n",
        "Make a few plots to get an overview of the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTN3n8OFTJf8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TF5I_YqQTJ1o"
      },
      "source": [
        "##Part 5.2.3 - Create a sub-set of the data for analysis\n",
        "Create a sub-set of each dataset containing only the data between September and November 2015.\n",
        "\n",
        "Hint: set the dataset indexes to the time column as we did in session 3 and use .loc to easily select time ranges."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CxOtBn2DbaXx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jL1vcb6rUas3"
      },
      "source": [
        "## Part 5.2.4 - Check the number of samples/sampling rate\n",
        "To do further analysis, it helps if all three datasets have the same amount of data with corresponding timestamps and sampling rate. Check the lenght of each and the sampling rate of each.\n",
        "\n",
        "Hint: you will need to using indexing, and the `DataFrame.value_counts()` method will be useful."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dNtY-dIbUagp"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DG15InoAecIg"
      },
      "source": [
        "# Part 5.3 - Timeseries and interpolation theory - *Mini-Lecture*\n",
        "You will see from the workshop that the rate of data sampling is different in each of the datasets, and is even inconsistent within some of them. To analyse them further, we need them to be at the same rate, so we need to resample or interpolate them.\n",
        "\n",
        "Super simple notes:\n",
        "\n",
        "**Point vs Period:** Data representing a particular location or moment in time, or across a range. E.g. \"daily\" temperature data = temperature at 12 noon each day, vs average temperature across 24 hours.\n",
        "\n",
        "**Resampling:** changing the positions or periods of data points (in e.g. time or spatial domain).\n",
        "\n",
        "**Interpolation:** how you \"get\" values for positions of periods which were not directly measured.\n",
        "\n",
        "**Upsampling:** less observations to more datapoints. E.g. creating daily data from weekly observations. Requires interpolation.\n",
        "\n",
        "**Downsampling:** more observations to less datapoints. E.g. creating quarterly data from monthly observations. Requires interpolation or aggregation.\n",
        "\n",
        "**Common interpolation methods:** linear, spline, nearest\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tseaDm4kgk-t"
      },
      "source": [
        "# Part 5.4 - Introduction to SciPy - *Mini-lecture*\n",
        "While Numpy handles multidimensional arrays and various mathematical operations on them, the SciPy library provides a wide range of more advanced functionality which is particularly useful for scientific data analysis.\n",
        "\n",
        "It includes for example modules concerning linear algebra, regression/fitting, integration, signal processing, image manipulation and statistics.\n",
        "\n",
        "SciPy can also refer to a collection of related libraries including Numpy, Pandas, Matplotlib and the SciPy library itself.\n",
        "\n",
        "It contains a module called scipy.interpolate which we will use in the next section.\n",
        "\n",
        "See here:\n",
        "https://www.scipy.org/scipylib/index.html\n",
        "\n",
        "The SciPy cookbook has many useful examples of using SciPy functions:\n",
        "https://scipy-cookbook.readthedocs.io/\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yj9tKXMPgxWR"
      },
      "source": [
        "#Part 5.5 - Cave monitoring data excercise part 2 (resampling/interpolation) - *Workshop*\n",
        "We will now resample the datasets so they have the same sampling rate, and that the data corresponds directly in terms of sampling time. We will resample everything to the same sampling rate and times as the cave temperature record."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rsIk7P6_kO7v"
      },
      "source": [
        "# For all of the below sections we will need the SciPy Interpolate module:\n",
        "from scipy import interpolate"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VNj-z34PFQt2"
      },
      "source": [
        "## Part 5.5.1 - Upsampling outside air temperature\n",
        "The outside air temperature is sampled at a lower rate than the cave temperature, therefore we need to upsample the outside air temperature record.\n",
        "\n",
        "Re-write the following pseudo-code to perform resampling by linear interpolation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59ilfSVJFcHG"
      },
      "source": [
        "'''\n",
        "1. Create an interpolation object containing the outside air temperature data where:\n",
        "We use the object \"interp1d\" (actually a class) from the Scipy interpolation \n",
        "<A> is the time column of the outside air temp data\n",
        "   we cannot interpolate using times directly, so \"to_numpy().astype(float) is a trick to convert the times to decimal numbers\"\n",
        "<B> is the temperature column outside air temp data\n",
        "<C> is the type of interpolation - look at the in interp1d help documentation\n",
        "<D> is the variable name to give to the interpolation object\n",
        "'''\n",
        "# <D> = interpolate.interp1d(<A>.to_numpy().astype(float), <B>, kind=<C>)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-JnPvNbeiEWr"
      },
      "source": [
        "'''\n",
        "2. Perform the interpolation by providing the times from the cave temperature\n",
        "data set. These are the times where we want to calculate surface air temperatures.\n",
        "<E> is the time column of the cave temp record, converted to decimal numbers as\n",
        "     in <A>\n",
        "<F> is the resampled time-series\n",
        "'''\n",
        "#<F> = <D>(<E>)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fxcf3y9Ii4Dz"
      },
      "source": [
        "'''\n",
        "3. Plot the resampled result and compare it to the original.\n",
        "It will be helpful to visualise the datapoints themselves, so use data markers\n",
        "in the plot.\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15APgdD3jssA"
      },
      "source": [
        "##Part 5.5.2 - Downsample the cave CO2 data\n",
        "The cave CO2 data is at a much higher and variable sampling rate, so we will downsample it to the cave temperature sampling rate/timings.\n",
        "\n",
        "Use the previous section as a guide. You will need to separately interpolate the CO2 concentration, the d18O and d13C data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jB8XGG_vkGrO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wupw6hB6l7j5"
      },
      "source": [
        "##Part 5.5.3 - Visualise data relationships.\n",
        "Now the data is directly comparable, we can easily create some simple visualisations (or calculate statistics).\n",
        "\n",
        "Try making a scatter plot(s) of CO2 concentration vs surface temperature, cave temperature, and the difference between cave and surface temperature."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XzH6-dD5WyA1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L22LdVA_kHD3"
      },
      "source": [
        "## Part 5.5.4 - Test interpolation methods - *Workshop*\n",
        "A number of interpolation methods exist besides linear interpolation. Experiment with and compare the different methods provided by interp1d. Which do you think are most appropriate in this context for the up- and downsampling and why?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t4GfjIBSkK7R"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghM9VJMPseX-"
      },
      "source": [
        "## Part 5.5.5 - Optional: Try out Pandas resampling and interpolation methods - *Workshop*\n",
        "Pandas has DataFrame methods `.resample()` and `.interpolate()` which make resampling very easy (although hide important details for learning what is going on).\n",
        "\n",
        "If you have time, try out the direct Pandas based approach.\n",
        "\n",
        "See documentation:\n",
        "\n",
        "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.resample.html\n",
        "\n",
        "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.interpolate.html\n",
        "\n",
        "And examples here:\n",
        "\n",
        "https://machinelearningmastery.com/resample-interpolate-time-series-data-python/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WmO6uBAjecgr"
      },
      "source": [
        "#Part 5.6 - Introduction to data filtering - *Mini-lecture*\n",
        "\n",
        "* Most common: moving/rolling averages\n",
        "* Make a moving/rolling \"window\", and take average of values in window for each position across dataset.\n",
        "* Simple application in Numpy:\n",
        " * Filter of 1/n (n=filter length)\n",
        " * convolve filter with data\n",
        "\n",
        "* Pandas has simple rolling/smoothing methods to explore yourselves.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6OqJlpUZiFDH"
      },
      "source": [
        "#Part 5.7 - Data filtering excercise - *Workshop*\n",
        "We will apply a simple moving average filter on a noisy dataset to smooth it.\n",
        "\n",
        "You can either use a dataset of your choice (from the student submitted datasets), or use the following XRF core scanning data from an Antarctic sediment core: \"https://doi.pangaea.de/10.1594/PANGAEA.859980?format=textfile\" (V is particularly noisy).\n",
        "\n",
        "1. Construct a filter: a 1d numpy array with length n and each value being 1/n (so its sum is 1). The length must be an odd number.\n",
        "\n",
        "2. Apply the filter by convolving it with the data to be filtered using np.convolve.\n",
        "\n",
        "3. Plot the original and smoothed data.\n",
        "\n",
        "4. Experiment with different filter lengths.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lc8Rc8D3fmo6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AdlfQJc8jQ-G"
      },
      "source": [
        "# Part 5.8 - Advanced filtering - *Mini-lecture*\n",
        "* Moving average filters equally weight importance of all data in the window.\n",
        "* However, probably want to weight central points more than distant ones.\n",
        "* Many more complex filter shapes exist, e.g. those in assignment task 1.\n",
        "* Filtering is also possible in frequency domain with Fourier Transforms â not addressed in this course.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4KbUjU_iU4I"
      },
      "source": [
        "# Part 5.9 - Week 5 Assignment\n",
        "\n",
        "**Task 1**\n",
        "\n",
        "Numpy provides some more complex filters here: https://numpy.org/doc/stable/reference/routines.window.html (window means filter in this context)\n",
        "Create a notebook where you apply these filters as well moving average filters to a noisy dataset. Experiment also with different filter lengths and comment on the differences and value of each method.\n",
        "\n",
        "**Task 2**\n",
        "\n",
        "Downsampling a noisy dataset by a significant amount (e.g. 10-100x) either by interpolation or just taking every nth data point, often leads to an unrepresentative representation of the original data. One solution is to first filter the data, then downsample the filtered dataset.\n",
        "\n",
        "Look back to the NGRIP dataset from Session 1. Resample it to 1 sample every 1 ky. First use the method we used above for the cave CO2 data, then try pre-filtering it (try different filters/sizes). Compare and discuss the difference between the two approaches.\n",
        "\n",
        "**Submission**\n",
        "* Submit the assignment here: https://github.com/ds4geo/ds4geo_ws2020/tree/master/Assignments/Session%203\n",
        "* Create a new Colab notebook via Google Drive, then save it to the submission repository using \"save a copy to GitHub\". See here:\n",
        " * https://github.com/ds4geo/ds4geo/blob/master/Github%20Assignment%20Readme.md\n",
        "* The **deadline** is 23:59 on 10th November 2020.\n",
        "* This assignment comprises 5% of the assessment for the course. Marks are split between the two tasks and are awarded for effectively running the analyses, visualising, and discussing the results.\n"
      ]
    }
  ]
}